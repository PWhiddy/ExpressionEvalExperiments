# this experiment does not work well because the sensitivity
# of each input feature with respect to the output varies by orders of magnitude!

import numpy as np
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data.dataset import Dataset
import argparse

#embedding_size = 16


def getIndex(dig):
    if (dig == ' '):
        return 10
    if (dig == '+'):
        return 11
    if (dig == '-'):
        return 12
    if (dig == '*'):
        return 13
    return int(dig)

def encodeNum(num, embed_size=16):
    if (len(num) > embed_size):
        print('ERROR!!!!! too many chars!!!')
    num = num + ' ' * (16-len(num))
    indexes = [getIndex(dg) for dg in num]
    indexes = torch.tensor(indexes)
    return torch.nn.functional.one_hot(indexes, embed_size).reshape(-1)

def makeNDigit(n):
    val = random.randint(0,(10**n)-1)
    encoded = encodeNum(str(val))
    return (encoded, torch.tensor([float(val)]))

class SimpleDigits(Dataset):
            
    def __init__(self, train=True):
        
        print('generating data...', flush=True)
        
        digit_count = 6
        total_size = 30000
        full_dataset = [makeNDigit(digit_count) for _ in range(total_size)]
        
        train_size = int(0.8 * len(full_dataset))
        test_size = len(full_dataset) - train_size
        print('train size: ' + str(train_size))
        print('test size: ' + str(test_size))
        train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])
        
        if (train):
            self.current_data = train_dataset
        else:
            self.current_data = test_dataset

    def __getitem__(self, index):
        return self.current_data[index] #(img, label)

    def __len__(self):
        return len(self.current_data)
    
class EvalNet(nn.Module):
    def __init__(self):
        super(EvalNet, self).__init__()
        self.fc1 = nn.Linear(256,256)
        self.fc3 = nn.Linear(512,512)
        self.fc2 = nn.Linear(512,1)
        #self.drop = nn.Dropout(p=0.55)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        #x = self.drop(x)
        pw = x*x
        x = torch.cat((x,pw), dim=1)
        x = F.relu(self.fc3(x))
        x = self.fc2(x)
        return x
        #x = self.drop(x)
        #return F.log_softmax(x, dim=1)
    
def train(args, model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device, torch.double), target.to(device, torch.double)
        optimizer.zero_grad()
        output = model(data)
        #loss = F.nll_loss(output, target)
        loss = F.l1_loss(output, target)
        #delt = output-target
        #delt = torch.log(delt*delt+1)
        #loss = delt.mean()
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            
def test(args, model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device, torch.double), target.to(device, torch.double)
            output = model(data)
            #test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            test_loss += F.l1_loss(output, target, reduction='sum').item()
            #delt = output-target
            #delt = torch.log(delt*delt+1)
            #test_loss += delt.sum().item()
            #pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            #correct += pred.eq(target.view_as(pred)).sum().item()
            deltas = torch.abs(output-target)
            correct += (deltas < 1.0).sum()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    
    
def main():
    # Training settings
    parser = argparse.ArgumentParser(description='expression eval test')
    parser.add_argument('--batch-size', type=int, default=1024, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=1200, metavar='N',
                        help='number of epochs to train (default: 10)')
    parser.add_argument('--lr', type=float, default=0.000024, metavar='LR',
                        help='learning rate (default: 0.01)')
    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
                        help='SGD momentum (default: 0.5)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=225, metavar='N',
                        help='how many batches to wait before logging training status')
    
    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
    args = parser.parse_args()
    use_cuda = not args.no_cuda and torch.cuda.is_available()

    torch.manual_seed(args.seed)
    random.seed(args.seed)

    device = torch.device("cuda" if use_cuda else "cpu")

    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    
    train_loader = torch.utils.data.DataLoader(
        SimpleDigits(train=True),
        batch_size=args.batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(
        SimpleDigits(train=False),
        batch_size=args.test_batch_size, shuffle=True, **kwargs)

    model = EvalNet().to(device, torch.double)
    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

    print("Beginning training..", flush=True)
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        test(args, model, device, test_loader)
        
    if (args.save_model):
        torch.save(model.state_dict(),"eval_test.pt")
        #torch.save(model,"keywords_sexuality_test.pt")
        
        
if __name__ == '__main__':
    main()
